{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Facebook Post Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "As one of the largest social media websites in the world, Facebook is an attractive platform for businesses to reach their consumers. Almost all consumer-facing businesses have virtual presence on Facebook, in the form of Facebook business pages (e.g., see [here](https://www.facebook.com/target/) for Target's Facebook business page). Everyday, Facebook users who visit these business pages generate a large amount of posts. These user posts may represent customer complains, questions, or appreciations directed towards the focal businesses. \n",
    "\n",
    "For businesses, these user posts contain valuable information about customers' needs and preferences, and understanding what the user posts are talking about represents an important opportunity to get to know your customers in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Task\n",
    "\n",
    "For this assignment, you will use a **labeled dataset** named \"FB_posts_labeled.txt\". It is a **tab-delimited** file with the following fields:\n",
    "- postId: this is a unique identifier for each user post. There are 7961 posts in total;\n",
    "- message: this is the text of each post;\n",
    "- Appreciation: this is a binary (0/1) indicator of whether a post is an appreciation;\n",
    "- Complaint: this is a binary (0/1) indicator of whether a post is a customer complaint;\n",
    "- Feedback: this is a binary (0/1) indicator of whether a post is a customer feedback (e.g., questions and suggestions).\n",
    "\n",
    "Appreciation, Complaint, and Feedback are the three mutually exclusive content categories / classes in this dataset. They were labeled by humans, and the labeling isn't perfect (i.e., there may be ambiguous cases where the labels are not appropriate). However, for the sake of this assignment, let's treat them as the ground truth. **Your task is to build a text classifier to predict the content category of a post based on its textual content.** \n",
    "\n",
    "To evaluate the out-of-sample performance of your model, you will use it to make predictions for 2039 posts in an **unlabeled dataset** named \"FB_posts_unlabeled.txt\". It is also a tab-delimited file, but only has postId and message fields. I keep the ground truth labels for these posts in a private place, in order to objectively evaluate your model's performance. The performance metric I will use is **averaged F-measure** across the three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = []\n",
    "# label = []\n",
    "# for line in open(\"FB_posts_labeled.txt\"):\n",
    "#     line = line.rstrip('\\n').split('\\t')\n",
    "#     text.append(line[0])\n",
    "#     label.append(int(line[1]))\n",
    "# text = np.array(text)\n",
    "# label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>message</th>\n",
       "      <th>Appreciation</th>\n",
       "      <th>Complaint</th>\n",
       "      <th>Feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126016648090_10150802142013091</td>\n",
       "      <td>Great ! ;)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108381603303_10151136215833304</td>\n",
       "      <td>YUM! YUM!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108381603303_3913438087739</td>\n",
       "      <td>Yummm :))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110455108974424_343049739048292</td>\n",
       "      <td>sweet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110455108974424_350358541650745</td>\n",
       "      <td>nice</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7956</th>\n",
       "      <td>179590995428478_390650150989227</td>\n",
       "      <td>Oregon locations</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7957</th>\n",
       "      <td>6806028948_10151298595908949</td>\n",
       "      <td>Just had two very long and very expensive flig...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7958</th>\n",
       "      <td>77978885595_10152286635360596</td>\n",
       "      <td>pet smart  #1756 Flowery branch ga really gave...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7959</th>\n",
       "      <td>125472670805257_397218893630632</td>\n",
       "      <td>having terrible trouble getting help from delt...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7960</th>\n",
       "      <td>117497138610_10150605733353611</td>\n",
       "      <td>i have no longer shop at your stores any more ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7961 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               postId  \\\n",
       "0      126016648090_10150802142013091   \n",
       "1      108381603303_10151136215833304   \n",
       "2          108381603303_3913438087739   \n",
       "3     110455108974424_343049739048292   \n",
       "4     110455108974424_350358541650745   \n",
       "...                               ...   \n",
       "7956  179590995428478_390650150989227   \n",
       "7957     6806028948_10151298595908949   \n",
       "7958    77978885595_10152286635360596   \n",
       "7959  125472670805257_397218893630632   \n",
       "7960   117497138610_10150605733353611   \n",
       "\n",
       "                                                message  Appreciation  \\\n",
       "0                                            Great ! ;)             1   \n",
       "1                                             YUM! YUM!             1   \n",
       "2                                             Yummm :))             1   \n",
       "3                                                 sweet             1   \n",
       "4                                                  nice             1   \n",
       "...                                                 ...           ...   \n",
       "7956                                   Oregon locations             0   \n",
       "7957  Just had two very long and very expensive flig...             0   \n",
       "7958  pet smart  #1756 Flowery branch ga really gave...             0   \n",
       "7959  having terrible trouble getting help from delt...             0   \n",
       "7960  i have no longer shop at your stores any more ...             0   \n",
       "\n",
       "      Complaint  Feedback  \n",
       "0             0         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             0         0  \n",
       "...         ...       ...  \n",
       "7956          1         0  \n",
       "7957          1         0  \n",
       "7958          1         0  \n",
       "7959          1         0  \n",
       "7960          1         0  \n",
       "\n",
       "[7961 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('FB_posts_labeled.txt', delimiter = '\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_target(x):\n",
    "    \n",
    "    if x['Appreciation'] == 1:\n",
    "        return '0'\n",
    "    elif x['Complaint'] == 1:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "    \n",
    "df['target'] = df.apply(lambda x: map_target(x), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4255\n",
       "0    2062\n",
       "2    1644\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "resample_df = df[['message', 'target']]\n",
    "X = resample_df['message']\n",
    "y = resample_df['target']\n",
    "ros = RandomOverSampler(random_state=42,sampling_strategy={'0':2500, '1': 4255, '2': 2000})\n",
    "X_res, y_res = ros.fit_resample(X.to_numpy().reshape(-1, 1), y)\n",
    "text = X_res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.keras.utils.to_categorical(y_res, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['message'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.keras.utils.to_categorical(df['target'], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8755,), (8755, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens = None,\n",
    "    standardize = 'lower_and_strip_punctuation',\n",
    "    split = 'whitespace',\n",
    "    ngrams = None,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19465"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply it to the text data with \"adapt\"\n",
    "vectorize_layer.adapt(text)\n",
    "# check preprocessing results, such as vocabulary, \n",
    "#vectorize_layer.get_vocabulary()\n",
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "array([[  95,  113, 2805],\n",
       "       [   1,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now use it to process some text\n",
    "input_text = [['very good movie'], ['Niharika']]\n",
    "vectorize_layer(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.Sequential()\n",
    "\n",
    "model_rnn.add(vectorize_layer)\n",
    "\n",
    "model_rnn.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_rnn.add(keras.layers.SimpleRNN(128)) # see note below\n",
    "\n",
    "model_rnn.add(keras.layers.Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_5 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, None, 64)          1245760   \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 128)               24704     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,270,851\n",
      "Trainable params: 1,270,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_rnn.compile(loss = keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer='adam',\n",
    "                  metrics=[[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 23s 68ms/step - loss: 0.7796 - precision_1: 0.7707 - recall_1: 0.5124 - val_loss: 1.0696 - val_precision_1: 0.3246 - val_recall_1: 0.2432\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 23s 74ms/step - loss: 0.4247 - precision_1: 0.8582 - recall_1: 0.7982 - val_loss: 0.7066 - val_precision_1: 0.7136 - val_recall_1: 0.6549\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 23s 71ms/step - loss: 0.2101 - precision_1: 0.9312 - recall_1: 0.9181 - val_loss: 0.1966 - val_precision_1: 0.9256 - val_recall_1: 0.9166\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 23s 72ms/step - loss: 0.1595 - precision_1: 0.9467 - recall_1: 0.9384 - val_loss: 0.2174 - val_precision_1: 0.9397 - val_recall_1: 0.9275\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 24s 74ms/step - loss: 0.0840 - precision_1: 0.9733 - recall_1: 0.9701 - val_loss: 0.0679 - val_precision_1: 0.9831 - val_recall_1: 0.9824\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 23s 73ms/step - loss: 0.1467 - precision_1: 0.9539 - recall_1: 0.9447 - val_loss: 0.1369 - val_precision_1: 0.9521 - val_recall_1: 0.9491\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 24s 74ms/step - loss: 0.0543 - precision_1: 0.9850 - recall_1: 0.9823 - val_loss: 0.1250 - val_precision_1: 0.9649 - val_recall_1: 0.9577\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 23s 72ms/step - loss: 0.0249 - precision_1: 0.9931 - recall_1: 0.9929 - val_loss: 0.0146 - val_precision_1: 0.9988 - val_recall_1: 0.9988\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 23s 70ms/step - loss: 0.0143 - precision_1: 0.9973 - recall_1: 0.9970 - val_loss: 0.0061 - val_precision_1: 0.9992 - val_recall_1: 0.9992\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 23s 73ms/step - loss: 0.0079 - precision_1: 0.9983 - recall_1: 0.9981 - val_loss: 0.0092 - val_precision_1: 0.9980 - val_recall_1: 0.9980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26602656760>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_rnn.fit(x = text, y = label, validation_split = 0.2,\n",
    "              epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 190ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.30253524, 0.41786057, 0.27960423]], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_rnn.predict(['timepass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.Sequential()\n",
    "\n",
    "model_lstm.add(vectorize_layer)\n",
    "\n",
    "model_lstm.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_lstm.add(keras.layers.LSTM(128)) # see note below\n",
    "\n",
    "model_lstm.add(keras.layers.Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_5 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, None, 64)          1245760   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               98816     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344,963\n",
      "Trainable params: 1,344,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_lstm.compile(loss = keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer='adam',\n",
    "                  metrics=[[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 59s 174ms/step - loss: 0.6617 - precision_2: 0.8348 - recall_2: 0.5811 - val_loss: 0.7268 - val_precision_2: 0.7379 - val_recall_2: 0.6616\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 57s 179ms/step - loss: 0.2839 - precision_2: 0.9175 - recall_2: 0.8861 - val_loss: 0.2464 - val_precision_2: 0.9454 - val_recall_2: 0.9150\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 57s 179ms/step - loss: 0.1457 - precision_2: 0.9574 - recall_2: 0.9467 - val_loss: 0.1784 - val_precision_2: 0.9490 - val_recall_2: 0.9405\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 57s 179ms/step - loss: 0.1105 - precision_2: 0.9687 - recall_2: 0.9622 - val_loss: 0.1293 - val_precision_2: 0.9670 - val_recall_2: 0.9632\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 58s 180ms/step - loss: 0.0706 - precision_2: 0.9796 - recall_2: 0.9766 - val_loss: 0.0403 - val_precision_2: 0.9867 - val_recall_2: 0.9859\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 57s 176ms/step - loss: 0.0461 - precision_2: 0.9854 - recall_2: 0.9833 - val_loss: 0.1134 - val_precision_2: 0.9646 - val_recall_2: 0.9616\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 59s 184ms/step - loss: 0.0555 - precision_2: 0.9837 - recall_2: 0.9817 - val_loss: 0.0565 - val_precision_2: 0.9855 - val_recall_2: 0.9839\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 58s 181ms/step - loss: 0.0304 - precision_2: 0.9913 - recall_2: 0.9901 - val_loss: 0.0979 - val_precision_2: 0.9761 - val_recall_2: 0.9749\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 59s 184ms/step - loss: 0.0209 - precision_2: 0.9934 - recall_2: 0.9922 - val_loss: 0.0256 - val_precision_2: 0.9925 - val_recall_2: 0.9914\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 58s 181ms/step - loss: 0.0128 - precision_2: 0.9963 - recall_2: 0.9955 - val_loss: 0.0117 - val_precision_2: 0.9973 - val_recall_2: 0.9961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2660309b970>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_lstm.fit(x = text, y = label, validation_split = 0.2,\n",
    "              epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 944ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2656086 , 0.39089018, 0.34350124]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_lstm.predict(['timepass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = keras.Sequential()\n",
    "\n",
    "model_gru.add(vectorize_layer)\n",
    "\n",
    "model_gru.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_gru.add(keras.layers.GRU(128)) # see note below\n",
    "\n",
    "model_gru.add(keras.layers.Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 64)          1245760   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               74496     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,320,643\n",
      "Trainable params: 1,320,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_gru.compile(loss = keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer='adam',\n",
    "                  metrics=[[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 85s 349ms/step - loss: 0.8012 - precision: 0.7437 - recall: 0.4993 - val_loss: 0.6713 - val_precision: 0.7842 - val_recall: 0.5957\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 68s 310ms/step - loss: 0.3989 - precision: 0.8777 - recall: 0.8225 - val_loss: 0.4651 - val_precision: 0.8564 - val_recall: 0.7933\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 69s 316ms/step - loss: 0.1973 - precision: 0.9385 - recall: 0.9256 - val_loss: 0.4725 - val_precision: 0.8520 - val_recall: 0.8218\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 68s 312ms/step - loss: 0.1169 - precision: 0.9663 - recall: 0.9589 - val_loss: 0.5140 - val_precision: 0.8743 - val_recall: 0.8538\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 72s 329ms/step - loss: 0.0738 - precision: 0.9787 - recall: 0.9753 - val_loss: 0.6444 - val_precision: 0.8551 - val_recall: 0.8389\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 71s 326ms/step - loss: 0.0680 - precision: 0.9795 - recall: 0.9767 - val_loss: 0.7092 - val_precision: 0.8251 - val_recall: 0.8030\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 68s 309ms/step - loss: 0.0640 - precision: 0.9821 - recall: 0.9789 - val_loss: 0.6611 - val_precision: 0.8591 - val_recall: 0.8464\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 71s 324ms/step - loss: 0.0280 - precision: 0.9927 - recall: 0.9914 - val_loss: 0.7860 - val_precision: 0.8552 - val_recall: 0.8435\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 72s 329ms/step - loss: 0.0215 - precision: 0.9947 - recall: 0.9940 - val_loss: 0.7985 - val_precision: 0.8591 - val_recall: 0.8464\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 70s 321ms/step - loss: 0.0118 - precision: 0.9970 - recall: 0.9967 - val_loss: 0.8786 - val_precision: 0.8569 - val_recall: 0.8447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaba0e8b50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_gru.fit(x = text, y = label, validation_split = 0.2,\n",
    "              epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4258635 , 0.23383796, 0.3402986 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_gru.predict(['timepass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - BiDirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bigru = keras.Sequential()\n",
    "\n",
    "model_bigru.add(vectorize_layer)\n",
    "\n",
    "model_bigru.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 128,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_bigru.add(keras.layers.Bidirectional(keras.layers.GRU(128, activation='relu')))\n",
    "\n",
    "#model_bigru.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "#model_bigru.add(keras.layers.Bidirectional(keras.layers.GRU(128, activation='relu')))\n",
    "\n",
    "model_bigru.add(keras.layers.Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_8 (Embedding)     (None, None, 128)         2491520   \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 256)              198144    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,690,435\n",
      "Trainable params: 2,690,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bigru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_bigru.compile(loss = keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer='adam',\n",
    "                  metrics=[[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 144s 644ms/step - loss: 0.7931 - precision_2: 0.7442 - recall_2: 0.5263 - val_loss: 0.4690 - val_precision_2: 0.8652 - val_recall_2: 0.7733\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 151s 691ms/step - loss: 0.3992 - precision_2: 0.8848 - recall_2: 0.8371 - val_loss: 0.3926 - val_precision_2: 0.8674 - val_recall_2: 0.8292\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 157s 714ms/step - loss: 0.1802 - precision_2: 0.9467 - recall_2: 0.9339 - val_loss: 0.4203 - val_precision_2: 0.8718 - val_recall_2: 0.8464\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 138s 633ms/step - loss: 0.0973 - precision_2: 0.9743 - recall_2: 0.9697 - val_loss: 0.4033 - val_precision_2: 0.8784 - val_recall_2: 0.8578\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 167s 765ms/step - loss: 0.0481 - precision_2: 0.9878 - recall_2: 0.9860 - val_loss: 0.5113 - val_precision_2: 0.8805 - val_recall_2: 0.8624\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 161s 735ms/step - loss: 0.0475 - precision_2: 0.9883 - recall_2: 0.9857 - val_loss: 0.6210 - val_precision_2: 0.8654 - val_recall_2: 0.8515\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 162s 742ms/step - loss: 0.0212 - precision_2: 0.9949 - recall_2: 0.9940 - val_loss: 0.7203 - val_precision_2: 0.8768 - val_recall_2: 0.8658\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 154s 704ms/step - loss: 0.0412 - precision_2: 0.9917 - recall_2: 0.9907 - val_loss: 0.5617 - val_precision_2: 0.8726 - val_recall_2: 0.8607\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 160s 731ms/step - loss: 0.0118 - precision_2: 0.9970 - recall_2: 0.9963 - val_loss: 0.7119 - val_precision_2: 0.8723 - val_recall_2: 0.8618\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 149s 683ms/step - loss: 0.0072 - precision_2: 0.9981 - recall_2: 0.9979 - val_loss: 0.8210 - val_precision_2: 0.8649 - val_recall_2: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212b1de8fa0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_bigru.fit(x = text, y = label, validation_split = 0.2,\n",
    "              epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 364ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.24583004, 0.31514028, 0.4390297 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_bigru.predict(['timepass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postId</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108381603303_10151119973393304</td>\n",
       "      <td>Love. It. To</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115568331790246_371841206162956</td>\n",
       "      <td>NICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115568331790246_515044031842672</td>\n",
       "      <td>Congrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147285781446_10151010892176447</td>\n",
       "      <td>Awesome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159616034235_10150639103634236</td>\n",
       "      <td>Award</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>179590995428478_422375854483323</td>\n",
       "      <td>you guys are terrible, holding goverment check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>125472670805257_525103854175468</td>\n",
       "      <td>as i platinum elite member of delta and a loya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>179590995428478_377568608964048</td>\n",
       "      <td>Really?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>179590995428478_341070505947192</td>\n",
       "      <td>Horrible decision.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>60686173217_10151073988283218</td>\n",
       "      <td>THIS IS A TRUE STORY OF WHAT HAPPENED TO ME TO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2039 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               postId  \\\n",
       "0      108381603303_10151119973393304   \n",
       "1     115568331790246_371841206162956   \n",
       "2     115568331790246_515044031842672   \n",
       "3      147285781446_10151010892176447   \n",
       "4      159616034235_10150639103634236   \n",
       "...                               ...   \n",
       "2034  179590995428478_422375854483323   \n",
       "2035  125472670805257_525103854175468   \n",
       "2036  179590995428478_377568608964048   \n",
       "2037  179590995428478_341070505947192   \n",
       "2038    60686173217_10151073988283218   \n",
       "\n",
       "                                                message  \n",
       "0                                          Love. It. To  \n",
       "1                                                  NICE  \n",
       "2                                              Congrats  \n",
       "3                                              Awesome!  \n",
       "4                                                 Award  \n",
       "...                                                 ...  \n",
       "2034  you guys are terrible, holding goverment check...  \n",
       "2035  as i platinum elite member of delta and a loya...  \n",
       "2036                                            Really?  \n",
       "2037                                 Horrible decision.  \n",
       "2038  THIS IS A TRUE STORY OF WHAT HAPPENED TO ME TO...  \n",
       "\n",
       "[2039 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_table('FB_posts_unlabeled.txt', delimiter = '\\t')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 3s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "target = model_bigru.predict(list(submission['message']))\n",
    "target_df = pd.DataFrame(target, columns = ['a','c','f'])\n",
    "target_df['label'] = target_df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Appreciation_pred'] = pd.get_dummies(target_df['label'])['a']\n",
    "submission['Complaint_pred'] = pd.get_dummies(target_df['label'])['c']\n",
    "submission['Feedback_pred'] = pd.get_dummies(target_df['label'])['f']\n",
    "submission.drop('message', axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('predictions_bi_gru_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your Predictions\n",
    "\n",
    "Throughout this assignment, you are encouraged to build different models and submit their predictions as many times as you'd like. To submit a set of predictions, you MUST adhere to the following format (a sample submission file that adheres to all the following requirements is provided on Canvas):\n",
    "\n",
    "1. The submission must be a csv file, with exactly four columns and 2040 rows;\n",
    "2. The first row must be the headers, specifically, \"postId,Appreciation_pred,Complaint_pred,Feedback_pred\". Spellings are case-sensitive;\n",
    "3. The first column must contain postId. The order of the posts doesn't matter - I will do a join between your predictions and the ground truth table based on postId;\n",
    "4. The remaining three columns contain your model's predictions for each post. Note that you must generate **binary predictions** for each category. In other words, the numbers in each of those three columns must be either 0 or 1. Also, a post can only belong to one category, so only 1 category can have value 1 and all the others must have value 0.\n",
    "\n",
    "Because I use an automated system to evaluate prediction performance, if your prediction file does not follow the above format, it won't be recognized. I suggest adapting the following pseudocode to generate the prediction file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run before adaptation, this is pseudocode!\n",
    "f = open('predictions.csv', 'w')\n",
    "# write the first header row\n",
    "f.write('postId,Appreciation_pred,Complaint_pred,Feedback_pred' + '\\n')\n",
    "\n",
    "for post in unlabeled_set:\n",
    "    Appreciation_pred, Complaint_pred, Feedback_pred = YOUR_MODEL_PREDICTIONS\n",
    "    if Appreciation_pred not in [0,1] or Complaint_pred not in [0,1] or Feedback_pred not in [0,1]:\n",
    "        SOMETHING_IS_WRONG (did you forget to turn probability predictions into binary predictions?)\n",
    "    if Appreciation_pred + Complaint_pred + Feedback_pred != 1:\n",
    "        SOMETHING_IS_WRONG\n",
    "    f.write(postId + ',' + str(Appreciation_pred) + ',' + str(Complaint_pred) + ',' + str(Feedback_pred) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To use the submission system**:\n",
    "1. Visit [http://18.189.32.82:3838/FBapp](http://18.189.32.82:3838/FBapp) to access the prediction submission system;\n",
    "2. Enter and select your x500 ID (because I need to keep track of who submitted what). You should see a text display \"welcome!\" after you enter your ID;\n",
    "3. Upload the prediction file with the correct format as discussed above. After the file is uploaded, the performance metrics will be shown automatically, including the precision/recall/F-measure of each class and the average F-measure. The entire confusion matrix is not provided to prevent gaming behavior.\n",
    "\n",
    "If the submission system is not working at any point during this assignment, please contact me via email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Your grade (out of 25 points) of this assignment is determined as follows:\n",
    "1. I rank everyone based on their highest performance. Say your rank is $A$;\n",
    "2. I rank everyone based on their second-highest performance. Say your rank is $B$;\n",
    "3. I rank everyone based on their third-highest performance. Say your rank is $C$;\n",
    "4. I compute a score (\"weighted average ranking\") $S = \\frac{1}{2}A + \\frac{1}{3}B + \\frac{1}{6}C$.\n",
    "5. The person(s) with the lowest $S$ gets 25 points, the person(s) with the second-lowest $S$ gets 24.5 points, so on and so forth.\n",
    "\n",
    "The design of this grading scheme **encourages consistent efforts that leads to steady performance improvement**, and demotes the relative importance of having one lucky high performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
